{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX+fggwIyEUAXpHW7az40e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/northpr/tensorflow-resources/blob/main/natural_language_processing/skimlit_pubmed_w_bert_glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skimlit with pretrained model (bert and glove)\n",
        "\n",
        "Source: [github](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/solutions/%F0%9F%9B%A0_09_Milestone_Project_2_SkimLit_%F0%9F%93%84%F0%9F%94%A5_Exercise_Solutions.ipynb)"
      ],
      "metadata": {
        "id": "8RfV-tUU--sM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data and preprocess"
      ],
      "metadata": {
        "id": "6O2XK5yG_QjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/northpr/helper_function/master/tensorflow/helper_tensorflow.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br3FB_Qj8BeY",
        "outputId": "a8fdd34e-1bcf-4908-f916-8df9cde5eb4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-09 14:40:25--  https://raw.githubusercontent.com/northpr/helper_function/master/tensorflow/helper_tensorflow.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7159 (7.0K) [text/plain]\n",
            "Saving to: ‘helper_tensorflow.py.1’\n",
            "\n",
            "\rhelper_tensorflow.p   0%[                    ]       0  --.-KB/s               \rhelper_tensorflow.p 100%[===================>]   6.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-09 14:40:25 (76.7 MB/s) - ‘helper_tensorflow.py.1’ saved [7159/7159]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
        "!ls pubmed-rct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-owomZ3W_Xp9",
        "outputId": "b53a43bb-688c-40a4-97d7-5b3667bd132d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pubmed-rct' already exists and is not an empty directory.\n",
            "PubMed_200k_RCT\n",
            "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
            "PubMed_20k_RCT\n",
            "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
            "README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start by using the 20k dataset\n",
        "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
        "\n",
        "# Check all of the filenames in the target directory\n",
        "import os\n",
        "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8JLGbIw_Z0w",
        "outputId": "182b4213-9bfa-42d1-bc56-01a893d472bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n",
              " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n",
              " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to read the lines of a document\n",
        "def get_lines(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    return f.readlines()\n",
        "\n",
        "    # Creating a preprocessing function that returns a dictionary\n",
        "def preprocess_text_with_line_numbers(filename):\n",
        "  \"\"\"Returns a list of dictionaries of abstract line data.\n",
        "\n",
        "  Takes in filename, reads its contents and sorts through each line,\n",
        "  extracting things like the target label, the text of the sentence,\n",
        "  how many sentences are in the current abstract and what sentence number\n",
        "  the target line is.\n",
        "\n",
        "  Args:\n",
        "      filename: a string of the target text file to read and extract line data\n",
        "      from.\n",
        "\n",
        "  Returns:\n",
        "      A list of dictionaries each containing a line from an abstract,\n",
        "      the lines label, the lines position in the abstract and the total number\n",
        "      of lines in the abstract where the line is from. For example:\n",
        "\n",
        "      [{\"target\": 'CONCLUSION',\n",
        "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
        "        \"line_number\": 8,\n",
        "        \"total_lines\": 8}]\n",
        "  \"\"\"\n",
        "  input_lines = get_lines(filename) # get all lines from filename\n",
        "  abstract_lines = \"\" # create an empty abstract\n",
        "  abstract_samples = [] # create an empty list of abstracts\n",
        "  \n",
        "  # Loop through each line in target file\n",
        "  for line in input_lines:\n",
        "    if line.startswith(\"###\"): # check to see if line is an ID line\n",
        "      abstract_id = line\n",
        "      abstract_lines = \"\" # reset abstract string\n",
        "    elif line.isspace(): # check to see if line is a new line\n",
        "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
        "\n",
        "      # Iterate through each line in abstract and count them at the same time\n",
        "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "        line_data = {} # create empty dict to store data from line\n",
        "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "        line_data[\"target\"] = target_text_split[0] # get target label\n",
        "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
        "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
        "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
        "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
        "    \n",
        "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
        "      abstract_lines += line\n",
        "  \n",
        "  return abstract_samples"
      ],
      "metadata": {
        "id": "m2WHXmrqAz-U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") \n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
        "\n",
        "len(train_samples), len(val_samples), len(test_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuNSmERdA3MT",
        "outputId": "476c9eac-c818-4f6e-883f-a5bd3be2fff1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 679 ms, sys: 104 ms, total: 783 ms\n",
            "Wall time: 1e+03 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading our data into a dataframe\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head(14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "kJRZiGKkA5Fu",
        "outputId": "9adc6983-fcd0-4ae1-ab9b-d4b91fee54fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         target                                               text  \\\n",
              "0     OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n",
              "1       METHODS  a total of @ patients with primary knee oa wer...   \n",
              "2       METHODS  outcome measures included pain reduction and i...   \n",
              "3       METHODS  pain was assessed using the visual analog pain...   \n",
              "4       METHODS  secondary outcome measures included the wester...   \n",
              "5       METHODS  serum levels of interleukin @ ( il-@ ) , il-@ ...   \n",
              "6       RESULTS  there was a clinically relevant reduction in t...   \n",
              "7       RESULTS  the mean difference between treatment arms ( @...   \n",
              "8       RESULTS  further , there was a clinically relevant redu...   \n",
              "9       RESULTS  these differences remained significant at @ we...   \n",
              "10      RESULTS  the outcome measures in rheumatology clinical ...   \n",
              "11  CONCLUSIONS  low-dose oral prednisolone had both a short-te...   \n",
              "12   BACKGROUND  emotional eating is associated with overeating...   \n",
              "13   BACKGROUND  yet , empirical evidence for individual ( trai...   \n",
              "\n",
              "    line_number  total_lines  \n",
              "0             0           11  \n",
              "1             1           11  \n",
              "2             2           11  \n",
              "3             3           11  \n",
              "4             4           11  \n",
              "5             5           11  \n",
              "6             6           11  \n",
              "7             7           11  \n",
              "8             8           11  \n",
              "9             9           11  \n",
              "10           10           11  \n",
              "11           11           11  \n",
              "12            0           10  \n",
              "13            1           10  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2552f66a-8999-470f-a44b-873ef435048e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>line_number</th>\n",
              "      <th>total_lines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>a total of @ patients with primary knee oa wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>outcome measures included pain reduction and i...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>pain was assessed using the visual analog pain...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>secondary outcome measures included the wester...</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>there was a clinically relevant reduction in t...</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the mean difference between treatment arms ( @...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>further , there was a clinically relevant redu...</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>these differences remained significant at @ we...</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the outcome measures in rheumatology clinical ...</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CONCLUSIONS</td>\n",
              "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>emotional eating is associated with overeating...</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>yet , empirical evidence for individual ( trai...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2552f66a-8999-470f-a44b-873ef435048e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2552f66a-8999-470f-a44b-873ef435048e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2552f66a-8999-470f-a44b-873ef435048e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert abstract text lines into lists \n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVOylFJiBXAI",
        "outputId": "43a3f735-c56a-4859-8180-36a472b0bddc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OneHotEncoder"
      ],
      "metadata": {
        "id": "_MTM1WWtDiYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# One hot encoding the labels \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_one_hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_wYp4bVDfG6",
        "outputId": "6e499922-d1a2-47d0-ba7c-d916316bac30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LabelEncoder"
      ],
      "metadata": {
        "id": "qMivowzVDn-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels and encoder them into integers \n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "\n",
        "label_encoder = LabelEncoder() \n",
        "\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVi7voM1DgH7",
        "outputId": "f8a4c0e7-d41f-427a-b2b8-86b88cd6da2e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 2, ..., 4, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names and number of classes from LabelEncoder instance \n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes , class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcsKgcm5Dhif",
        "outputId": "c1eb1ebd-217d-479c-e1ab-8d3cadeef064"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Glove embeddings"
      ],
      "metadata": {
        "id": "qvchlvEYDr-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pre-trained embeddings \n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuMlE9RHDudk",
        "outputId": "ced161c7-4c2e-46ad-fb99-1eaab2848be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-09 14:40:31--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-10-09 14:40:31--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-10-09 14:40:31--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.08MB/s    in 2m 46s  \n",
            "\n",
            "2022-10-09 14:43:18 (4.96 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the path of the glove embedding (using 100D)\n",
        "import numpy as np \n",
        "glove_path = 'glove.6B.100d.txt'\n",
        "\n",
        "embedding_index = {}\n",
        "\n",
        "# Making dict of vector representtion of the words (s --> [8, 48......])\n",
        "with open(glove_path) as f:\n",
        "  for line in f:\n",
        "    \n",
        "    # Getting the words and coef in a variable \n",
        "    word , coefs = line.split(maxsplit = 1)\n",
        "    coefs = np.fromstring(coefs , 'f' , sep = ' ')\n",
        "    \n",
        "    # Adding the coefs to our embedding dict \n",
        "    embedding_index[word] = coefs\n",
        "\n",
        "print(f'Found {len(embedding_index)} word vectors')"
      ],
      "metadata": {
        "id": "Rz9Bwh2mDwFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make function to split sentence into characters\n",
        "def split_chars(text):\n",
        "  return \" \".join(list(text))\n",
        "\n",
        "# Split sequence-level data splits into character-lelve data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]"
      ],
      "metadata": {
        "id": "7z2fMaljFSPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a text vectorization layer (68k vocab size from the paper itself)\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=68000,\n",
        "                                    output_sequence_length=56)\n",
        "\n",
        "# Adapt our text vectorizer to training sentences\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "WTM7ocRzFyCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the vocabulary of the vectorizer\n",
        "text_vocab = text_vectorizer.get_vocabulary()\n",
        "len(text_vocab), text_vocab[0:20]"
      ],
      "metadata": {
        "id": "cujLZ9iEF3T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For glove embedding"
      ],
      "metadata": {
        "id": "UndUUkORHP-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the dict mapping word --> index \n",
        "word_index_text = dict(zip(text_vocab , range(len(text_vocab))))\n",
        "word_index_text"
      ],
      "metadata": {
        "id": "Vl1iryULGpvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function that will give us. a embedding matrix\n",
        "def get_glove_embedding_matrix(num_tokens, embedding_dim, word_index):\n",
        "\n",
        "  # Defining the hits and misses here\n",
        "  hits, misses = 0,0\n",
        "\n",
        "  # Prepare the embedding matrix\n",
        "  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      hits += 1\n",
        "    else:\n",
        "      misses += 1\n",
        "\n",
        "  return embedding_matrix, hits, misses"
      ],
      "metadata": {
        "id": "X_nDf3epG6IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_text = len(text_vocab)+2 # All number of the words\n",
        "embedding_dim = 100\n",
        "\n",
        "sentence_embedding_matrix , hits_ , misses_ = get_glove_embedding_matrix(num_tokens_text , embedding_dim, word_index_text)\n",
        "\n",
        "print(f'Hits: {hits_} and Misses: {misses_} for the sentence embedding matrix')\n"
      ],
      "metadata": {
        "id": "mVjfBrjoHhuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the embedding matrix to our Embedding layer (Sentence and characters)\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "sen_embedding_layer = Embedding(num_tokens_text , \n",
        "                            embedding_dim , \n",
        "                            embeddings_initializer = tf.keras.initializers.Constant(sentence_embedding_matrix) , \n",
        "                            trainable = False,\n",
        "                            name=\"glove_embedding\")"
      ],
      "metadata": {
        "id": "7sFGwKWAMgNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the sentence before and after all the vectorization and embedding\n",
        "import random\n",
        "\n",
        "ran_num = random.randint(0,len(train_sentences))\n",
        "random_sentence = train_sentences[ran_num]\n",
        "\n",
        "print(f\"Example of train sentence:\\n{random_sentence}\")\n",
        "\n",
        "print(\"=======\\n\")\n",
        "print(f\"Text vectorization:\\n{text_vectorizer(random_sentence)}\")\n",
        "print(f\"Text vectorization shape: {text_vectorizer(random_sentence).shape}\")\n",
        "\n",
        "print(\"=======\\n\")\n",
        "print(f\"Embedded Text:\\n{sen_embedding_layer(text_vectorizer(random_sentence))}\\n\")\n",
        "print(f\"Embedded text shape: {sen_embedding_layer(text_vectorizer(random_sentence)).shape}\")\n"
      ],
      "metadata": {
        "id": "3dWfojCd9GON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the datasets for our both sentences and chars  \n",
        "\n",
        "train_sen_vectors = text_vectorizer(np.array([[sen] for sen in train_sentences])).numpy()\n",
        "val_sen_vectors = text_vectorizer(np.array([[sen] for sen in val_sentences])).numpy()\n",
        "\n",
        "# Training and validation dataset \n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_sen_vectors , train_labels_encoded)) # For model 1\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_sen_vectors , val_labels_encoded))\n",
        "\n",
        "\n",
        "# Applying the batch size and prefetching (performance optimization )\n",
        "train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "train_ds,  val_ds"
      ],
      "metadata": {
        "id": "4twA5EwwMscr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the input\n",
        "train_sen_vectors.shape"
      ],
      "metadata": {
        "id": "DfxdGbakM0L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sen_vectors[0].shape"
      ],
      "metadata": {
        "id": "tLlgrUvfPbLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 0: Glove embedding test with sentences"
      ],
      "metadata": {
        "id": "nGyxZvZTAKC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
        "\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "c5Kp0YV6GCCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_sentences), len(train_labels_one_hot)"
      ],
      "metadata": {
        "id": "fSh9lM2iIzAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(123)\n",
        "\n",
        "# Sample\n",
        "inputs = layers.Input(shape= (1,), dtype=\"string\")\n",
        "text_vector = text_vectorizer(inputs)\n",
        "glove_emb = sen_embedding_layer(text_vector)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(glove_emb)\n",
        "x = layers.AveragePooling1D(5, padding=\"same\")(x)\n",
        "x = layers.Conv1D(64, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.AveragePooling1D(5, padding=\"same\")(x)\n",
        "x = layers.Conv1D(32, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "\n",
        "model_0 = tf.keras.Model(inputs, outputs, name=\"conv1d_global_average_pooling\")\n",
        "model_0.summary()"
      ],
      "metadata": {
        "id": "adUtUtlZAOje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=[\"accuracy\"])\n",
        "\n",
        "model_0.fit(train_dataset,\n",
        "            epochs=5,\n",
        "            validation_data=valid_dataset)"
      ],
      "metadata": {
        "id": "2EhvOHwiFcrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_0 = model_0.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "FnFx27VuLSeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: Glove embedding with GlobalAveragePooling"
      ],
      "metadata": {
        "id": "nb-HJfux_c-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(123)\n",
        "\n",
        "# Sample\n",
        "inputs = layers.Input(shape= (None,), dtype=\"int64\") # Data is already `int64` because we already vectorize it.\n",
        "glove_emb = sen_embedding_layer(inputs)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(glove_emb)\n",
        "x = layers.AveragePooling1D(5, padding=\"same\")(x)\n",
        "x = layers.Conv1D(64, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.AveragePooling1D(5, padding=\"same\")(x)\n",
        "x = layers.Conv1D(32, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "\n",
        "model_1 = tf.keras.Model(inputs, outputs, name=\"conv1d_global_average_pooling\")\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "FoUVRi2aNPHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=[\"accuracy\"])\n",
        "\n",
        "model_1.fit(train_ds,\n",
        "            epochs=5,   \n",
        "            validation_data = val_ds)"
      ],
      "metadata": {
        "id": "K-L_KGUW25xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_1 = model_1.evaluate(val_ds)"
      ],
      "metadata": {
        "id": "iOLk-N4N4J1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: Glove embedding with MaxPooling"
      ],
      "metadata": {
        "id": "B6Rxzo4-AsIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(123)\n",
        "\n",
        "# Sample\n",
        "inputs = layers.Input(shape= (None,), dtype=\"int64\") # Data is already `int64` because we already vectorize it.\n",
        "glove_emb = sen_embedding_layer(inputs)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(glove_emb)\n",
        "x = layers.MaxPooling1D(5, padding=\"same\")(x)\n",
        "x = layers.Conv1D(64, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling1D(5, padding=\"same\")(x)\n",
        "x = layers.Conv1D(32, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs, outputs, name=\"conv1d_global_average_pooling\")\n",
        "model_2.summary()"
      ],
      "metadata": {
        "id": "Ss3_AdRG8QkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=[\"accuracy\"])\n",
        "\n",
        "model_2.fit(train_ds,\n",
        "            epochs=5,   \n",
        "            validation_data = val_ds)"
      ],
      "metadata": {
        "id": "lyAMSWlKAyCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_2 = model_2.evaluate(val_ds)"
      ],
      "metadata": {
        "id": "pJlrLaSHEheb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snAD8vYWEoZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}